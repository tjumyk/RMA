{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import redis\n",
    "import json\n",
    "import time\n",
    "import sklearn.utils\n",
    "import psycopg2\n",
    "from psycopg2.sql import Identifier, SQL\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = redis.Redis(password='3oSYTdtZjsuSigRWLcG6VJt9gm4IMvYjQiqsSuGcAc-U4gMNpWGERAevXi9_SHNrn19piz7bBJG0iTLgx7DvknLHTECcHYrqmWb2rsuCWs89svKmhKDD_aMYaXq8IhSeg_89ooPZb0AqLRyR1-fa1zVjrh2UuV0sWFGSk5SjtW0', \n",
    "                   host='localhost', port=7380, decode_responses=True)\n",
    "conn_psql_kongzi2 = psycopg2.connect(database='aida', user='zding', password='dingzishuo', host='localhost',\n",
    "                                     port=5432)\n",
    "conn_psql = psycopg2.connect(database='zding', user='zding', password='dingzishuo', host='localhost',\n",
    "                                     port=5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_results(exp_id, d_t, data_source='aida_conll'):\n",
    "    fea_res = conn.lrange('result:::::' + str(exp_id) + ':::::' + data_source + ':::::' + str(d_t), 0, -1)\n",
    "    fea_vecs = [json.loads(res) for res in fea_res]\n",
    "    return fea_vecs\n",
    "\n",
    "def get_dataset_info(exp_id, d_t, data_source='aida_conll', can_size=50):\n",
    "    valid_mens_size = conn.scard('valid_qry_ids:::::' + str(exp_id) + ':::::' + data_source + ':::::' + str(d_t))\n",
    "    no_g_can_info_size = conn.scard('no_g_candidate_info:::::' + str(exp_id) + ':::::' + data_source + ':::::' + str(d_t))\n",
    "    missed_g_candidate_size = conn.scard('missed_g_candidate:::::' + str(exp_id) + ':::::' + data_source + \n",
    "                                         ':::::' + str(d_t) + ':::::' + str(can_size))\n",
    "    no_candidate_size = conn.scard('no_candidate:::::' + str(exp_id) + ':::::' + data_source + ':::::' + str(d_t))\n",
    "    return valid_mens_size, no_candidate_size, no_g_can_info_size, missed_g_candidate_size\n",
    "\n",
    "def fetch_all_features(exp_id, data_type, data_source='aida_conll'):\n",
    "    res_feas = get_feature_results(exp_id, data_type, data_source)\n",
    "    res_feas_ids = [[res[0].strip('(').split(', ')[0]] + res[1:-1] + [res[-1]] for res in res_feas]\n",
    "    res_feas_ids = np.array(res_feas_ids, dtype=np.float64)\n",
    "    return res_feas_ids\n",
    "\n",
    "def fetch_all_features_delete_max_prior(exp_id, data_type, data_source='aida_conll'):\n",
    "    res_feas = get_feature_results(exp_id, data_type, data_source)\n",
    "    res_feas_ids = [[res[0].strip('(').split(', ')[0]] + res[1:2] + [res[2] if res[2]!=0 else res[1]] + res[3:] for res in res_feas]\n",
    "    res_feas_ids = np.array(res_feas_ids, dtype=np.float64)\n",
    "    return res_feas_ids\n",
    "\n",
    "def trans_data(data):\n",
    "    d_np = data[:, 1:-1]\n",
    "    #print(d_np)\n",
    "    d_labels = data[:, -1]\n",
    "    #print(d_labels)\n",
    "    idxs = np.where(d_labels == 1)[0]\n",
    "    d_groups = np.append(np.delete(idxs, 0), len(d_labels)) - idxs\n",
    "    xgb_data = xgb.DMatrix(data=d_np, label=d_labels)\n",
    "    xgb_data.set_group(d_groups)\n",
    "    return xgb_data\n",
    "\n",
    "def combine_features(original_feas, new_features):\n",
    "    men_id_feas_dict = defaultdict(list)\n",
    "    print(\"Building idx for new features...\")\n",
    "    for fea in new_features:\n",
    "        men_id_feas_dict[fea[0]].append(fea)\n",
    "    #for k, v in men_id_feas_dict.items():\n",
    "    #    print(k)\n",
    "    #    print(v)\n",
    "    comb_feas = []\n",
    "    pre_men_id = 0\n",
    "    print(\"Combine original and new features...\")\n",
    "    for fea_idx, fea in enumerate(original_feas):\n",
    "        #print('Processing: [%d] %s' % (fea_idx, fea))\n",
    "        if pre_men_id == fea[0]:\n",
    "            #print('skip')\n",
    "            continue\n",
    "        else:\n",
    "            pre_men_id = fea[0]\n",
    "            fea_size = len(men_id_feas_dict[fea[0]])\n",
    "            res = np.append(original_feas[fea_idx: fea_idx + fea_size, :-1], np.array(men_id_feas_dict[fea[0]])[:, 1:], axis=1)\n",
    "            #print('res:', res)\n",
    "            comb_feas.append(res)\n",
    "    return np.concatenate(comb_feas, axis=0)\n",
    "\n",
    "def evalerror(preds, dt, d_tal_size):\n",
    "    d_l = dt.get_label()\n",
    "    idxs = np.where(d_l == 1)[0]\n",
    "    d_groups = np.append(np.delete(idxs, 0), len(d_l)) - idxs\n",
    "    matched_ids = []\n",
    "    q_id = 0\n",
    "    for x in d_groups:\n",
    "        pre_res = preds[q_id: x + q_id]\n",
    "        if(preds[q_id] == max(pre_res)):\n",
    "            if len([x for x in pre_res if x == preds[q_id]]) == 1:\n",
    "                matched_ids.append(q_id)\n",
    "        q_id += x\n",
    "    precision = float(len(matched_ids)) / len(d_groups)\n",
    "    recall = float(len(matched_ids)) / d_tal_size\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return len(matched_ids), precision, recall, f1\n",
    "\n",
    "from collections import defaultdict\n",
    "def evalerror_detail_log(preds, dt, d_tal_size):\n",
    "    d_l = dt.get_label()\n",
    "    idxs = np.where(d_l == 1)[0]\n",
    "    d_groups = np.append(np.delete(idxs, 0), len(d_l)) - idxs\n",
    "    correct_results = {}\n",
    "    wrong_results = {}\n",
    "    duplicates_results = {}\n",
    "    group_info = {}\n",
    "    matched_ids = []\n",
    "    q_id = 0\n",
    "    for x in d_groups:\n",
    "        pre_res = preds[q_id: x + q_id]\n",
    "        if(preds[q_id] == max(pre_res)):\n",
    "            correct_results[q_id] = pre_res\n",
    "            if len([x for x in pre_res if x == preds[q_id]]) == 1:\n",
    "                matched_ids.append(q_id)\n",
    "            else:\n",
    "                duplicates_results[q_id] = pre_res\n",
    "        else:\n",
    "            wrong_results[q_id] = pre_res\n",
    "        q_id += x\n",
    "    precision = float(len(matched_ids)) / len(d_groups)\n",
    "    recall = float(len(matched_ids)) / d_tal_size\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return len(matched_ids), precision, recall, f1, correct_results, wrong_results, duplicates_results\n",
    "\n",
    "from collections import defaultdict\n",
    "from ast import literal_eval\n",
    "def get_groups_results(preds, dt, res_features, top_k=None):\n",
    "    d_l = dt.get_label()\n",
    "    idxs = np.where(d_l == 1)[0]\n",
    "    d_groups = np.append(np.delete(idxs, 0), len(d_l)) - idxs\n",
    "    correct_res_groups = []\n",
    "    wrong_res_groups = []\n",
    "    dup_res_groups = []\n",
    "    top_k_indices = [] if top_k is not None else None\n",
    "    q_id = 0\n",
    "    for x in d_groups:\n",
    "        pre_res = preds[q_id: x + q_id]\n",
    "        pre_res_feas = res_features[q_id: x + q_id]\n",
    "        pred_q_id, pred_ent = literal_eval(res_features[q_id+np.argmax(pre_res)][0])\n",
    "        \n",
    "        if(preds[q_id] == max(pre_res)):\n",
    "            correct_res_groups.append([pred_q_id, pred_ent])\n",
    "        else:\n",
    "            wrong_res_groups.append([pred_q_id, pred_ent])\n",
    "            \n",
    "        if top_k is not None:  # save indices of top-k scores in each group\n",
    "            for i, score in sorted(enumerate(pre_res),key=lambda x:x[1], reverse=True)[:top_k]:\n",
    "                # print(i, score)\n",
    "                top_k_indices.append(i+q_id)\n",
    "            \n",
    "        q_id += x\n",
    "        \n",
    "    return correct_res_groups, wrong_res_groups, top_k_indices\n",
    "\n",
    "def fetch_inlinks_by_ent(ent):\n",
    "    cur = conn_psql_kongzi2.cursor()\n",
    "    sql = \"SELECT _id FROM wikipedia_links_2014 WHERE target=%s;\"\n",
    "    cur.execute(sql, (ent,))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    return rows\n",
    "\n",
    "# fetch an entity's outlinks with duplicates\n",
    "def fetch_outlinks_by_ent(ent):\n",
    "    cur = conn_psql_kongzi2.cursor()\n",
    "    sql = \"SELECT target FROM wikipedia_links_2014 WHERE _id=%s;\"\n",
    "    cur.execute(sql, (ent,))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    return rows\n",
    "\n",
    "def fetch_entity_by_mention_emnlp17(mention):\n",
    "    # print(mention)\n",
    "    cur = conn_psql_kongzi2.cursor()\n",
    "    # do a PostgreSQL join to select the entity namestring from the tables dictionary and entity_ids\n",
    "    sql = \"SELECT entity, prior FROM men_ent_dict_emnlp2017 WHERE men_ent_dict_emnlp2017.mention = (E\\'%s\\') ORDER BY prior DESC;\"\n",
    "    cur.execute(sql % mention.replace(\"'\", \"\\\\'\"))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    return rows\n",
    "\n",
    "def fetch_inlinks_redis(ent, link_type='inlinks'):\n",
    "    inlinks = conn.hmget(link_type, ent)[0]\n",
    "    return json.loads(inlinks) if inlinks else []\n",
    "\n",
    "def has_inlinks_redis(ent, link_type='inlinks'):\n",
    "    inlinks = conn.hexists(link_type, ent)\n",
    "    return inlinks\n",
    "\n",
    "def save_inlinks_redis(ent, inlinks, link_type='inlinks'):\n",
    "    conn.hset(link_type, ent, json.dumps(inlinks))\n",
    "    \n",
    "def fetch_outlinks_redis(ent, link_type='outlinks'):\n",
    "    outlinks = conn.hmget(link_type, ent)[0]\n",
    "    return json.loads(outlinks) if outlinks else []\n",
    "\n",
    "def has_outlinks_redis(ent, link_type='outlinks'):\n",
    "    inlinks = conn.hexists(link_type, ent)\n",
    "    return inlinks\n",
    "\n",
    "def save_outlinks_redis(ent, outlinks, link_type='outlinks'):\n",
    "    conn.hset(link_type, ent, json.dumps(outlinks))\n",
    "    \n",
    "def check_links_between_ents(ent_1, ent_2, bidirection=False):\n",
    "    wiki_pre_str = 'en.wikipedia.org/wiki/'\n",
    "    inlinks_ent_1 = fetch_inlinks_redis(ent_1, link_type='inlinks')\n",
    "    if not inlinks_ent_1 and not has_inlinks_redis(ent_1):\n",
    "        print(\"PostgreSQL: fetching inlinks for entity {}...\".format(ent_1))\n",
    "        wiki_ents_1 = wiki_pre_str + ent_1\n",
    "        inlinks_ent_1_db = fetch_inlinks_by_ent(wiki_ents_1)\n",
    "        inlinks_ent_1 = [x[0].replace(wiki_pre_str, '') for x in inlinks_ent_1_db]\n",
    "        print(\"Redis: caching inlinks for entity {}...\".format(ent_1))\n",
    "        save_inlinks_redis(ent_1, inlinks_ent_1)\n",
    "    inlinks_ent_2 = fetch_inlinks_redis(ent_2, link_type='inlinks')\n",
    "    if not inlinks_ent_2 and not has_inlinks_redis(ent_2):        \n",
    "        wiki_ents_2 = wiki_pre_str + ent_2\n",
    "        print(\"PostgreSQL: fetching inlinks for entity {}...\".format(ent_2))\n",
    "        inlinks_ent_2_db = fetch_inlinks_by_ent(wiki_ents_2)\n",
    "        inlinks_ent_2 = [x[0].replace(wiki_pre_str, '') for x in inlinks_ent_2_db]\n",
    "        print(\"Redis: caching inlinks for entity {}...\".format(ent_2))\n",
    "        save_inlinks_redis(ent_2, inlinks_ent_2)\n",
    "    return (ent_1 in inlinks_ent_2 or ent_2 in inlinks_ent_1) if not bidirection else (ent_1 in inlinks_ent_2 and ent_2 in inlinks_ent_1)\n",
    "\n",
    "def get_links_by_ent(ent, link_type='inlinks'):\n",
    "    wiki_pre_str = 'en.wikipedia.org/wiki/'\n",
    "    if link_type == 'inlinks':\n",
    "        inlinks_ent = fetch_inlinks_redis(ent, link_type='inlinks')\n",
    "        if not inlinks_ent and not has_inlinks_redis(ent):\n",
    "#             print(\"PostgreSQL: fetching inlinks for entity {}...\".format(ent))\n",
    "            wiki_ents = wiki_pre_str + ent\n",
    "            inlinks_ent_db = fetch_inlinks_by_ent(wiki_ents)\n",
    "            inlinks_ent = [x[0].replace(wiki_pre_str, '') for x in inlinks_ent_db]\n",
    "            print(\"Redis: caching inlinks for entity {}...\".format(ent))\n",
    "            save_inlinks_redis(ent, inlinks_ent)\n",
    "        return inlinks_ent\n",
    "    if link_type == 'outlinks':\n",
    "        outlinks_ent = fetch_outlinks_redis(ent)\n",
    "        if not outlinks_ent and not has_outlinks_redis(ent):\n",
    "#             print(\"PostgreSQL: fetching outlinks for entity {}...\".format(ent))\n",
    "            wiki_ents = wiki_pre_str + ent\n",
    "            outlinks_ent_db = fetch_outlinks_by_ent(wiki_ents)\n",
    "            outlinks_ent = [x[0].replace(wiki_pre_str, '') for x in outlinks_ent_db]\n",
    "            print(\"Redis: caching outlinks for entity {}...\".format(ent))\n",
    "            save_outlinks_redis(ent, outlinks_ent)\n",
    "        return outlinks_ent\n",
    "    \n",
    "def fetch_ents_by_doc_redis(doc_id):\n",
    "    id_ents = conn.hmget('doc-predicted-ents-coref-new', doc_id)[0]\n",
    "    return json.loads(id_ents) if id_ents else []\n",
    "\n",
    "## Normalized Google Distance\n",
    "import math\n",
    "def ngd_similarity(ents_s, ents_t, index_size = 6274625):\n",
    "    ent_sets_s = set(ents_s)\n",
    "    ent_sets_t = set(ents_t)\n",
    "    min_links, max_links = min(len(ent_sets_s), len(ent_sets_t)), max(len(ent_sets_s), len(ent_sets_t))\n",
    "    com_links = len(ent_sets_s & ent_sets_t)\n",
    "    if min_links and max_links and com_links:\n",
    "        return 1 - (math.log(max_links) - math.log(com_links))/ (math.log(index_size) - math.log(min_links))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# PMI\n",
    "def pmi_similarity(ents_s, ents_t, index_size = 6274625, normalize=False):\n",
    "    ent_sets_s = set(ents_s)\n",
    "    ent_sets_t = set(ents_t)\n",
    "    s_links, t_links = len(ent_sets_s), len(ent_sets_t)\n",
    "    com_links = len(ent_sets_s & ent_sets_t)\n",
    "    p_s = s_links / index_size\n",
    "    p_t = t_links / index_size\n",
    "    p_c = com_links / index_size\n",
    "    print(p_s, p_t, p_c)\n",
    "    if p_s and p_t and p_c:\n",
    "        return p_c/(p_s * p_t) if not normalize else p_c / (p_s * p_t) / min(1/p_s, 1/p_t)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_path = './new_models_14_Aug'\n",
    "def save_model(model, name):\n",
    "    if not os.path.exists(model_dir_path):\n",
    "        os.makedirs(model_dir_path)\n",
    "    model_path = os.path.join(model_dir_path, '%s.mdl' % name)\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "def load_model(name):\n",
    "    model_path = os.path.join(model_dir_path, '%s.mdl' % name)\n",
    "    with open(model_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def get_total_mentions(data_source, data_type) -> int:\n",
    "    with conn_psql.cursor() as cur:\n",
    "        sql = SQL(\"select count(*) from {} where annotation != 'NIL' and type=%s\").format(Identifier(data_source))\n",
    "        cur.execute(sql, (data_type,))\n",
    "        return cur.fetchone()[0]\n",
    "    \n",
    "def process(process_name ,test_set, test_total, \n",
    "            n_estimators, max_depths, test_filter=None,\n",
    "            eval_func=evalerror_detail_log):\n",
    "    if test_filter is not None:\n",
    "        dtest_xgboost = trans_data(test_set[test_filter])    \n",
    "    else:\n",
    "        dtest_xgboost = trans_data(test_set)\n",
    "    \n",
    "    for x in n_estimators:\n",
    "        num_round = x\n",
    "        for dep in max_depths:\n",
    "            model_name = '%d_%d_%s' % (num_round, dep, process_name)\n",
    "            print(datetime.now(), 'Loading model: %s' % model_name)\n",
    "            bst = load_model(model_name)\n",
    "\n",
    "            print(datetime.now(), 'Start evaluation')\n",
    "            preds = bst.predict(dtest_xgboost)\n",
    "            a = eval_func(preds, dtest_xgboost, test_total)\n",
    "            print(\"n_estimators: {}, max_depth: {}, acc_validation: {}, corr_num: {}\".format(num_round, dep, a[2], a[0]))\n",
    "            print(datetime.now(), 'Evaluation finished')\n",
    "            \n",
    "\n",
    "def fetch_q_ids_docs(data_source):\n",
    "    cur = conn_psql.cursor()\n",
    "    sql = \"SELECT id, doc_id FROM %s WHERE annotation != 'NIL';\" % data_source\n",
    "    cur.execute(sql)\n",
    "    row = cur.fetchall()\n",
    "    cur.close()\n",
    "    return dict(row)\n",
    "            \n",
    "def save_local_model_predictions(model, d_test, raw_test, docs_dict, top_k=None):\n",
    "    d_test_xgboost = trans_data(d_test)\n",
    "    preds_test = model.predict(d_test_xgboost)\n",
    "    \n",
    "    correct_test, wrong_test, top_k_indices_test = get_groups_results(preds_test, d_test_xgboost, raw_test, top_k)\n",
    "    res_all_test = correct_test + wrong_test\n",
    "    \n",
    "    print('Number of groups:', len(res_all_test))\n",
    "    \n",
    "    doc_id_q_ent_lists_dict = defaultdict(list)\n",
    "    for q_ent in res_all_test:\n",
    "        doc_id_q_ent_lists_dict[docs_dict[q_ent[0]]].append(q_ent)\n",
    "    \n",
    "    for key, vals in doc_id_q_ent_lists_dict.items():\n",
    "        conn.hset('doc-predicted-ents-coref-new-for-review', key, json.dumps(vals))\n",
    "    \n",
    "    if top_k is not None:\n",
    "        conn.hset('doc-predicted-ents-top-k', 'test', repr(top_k_indices_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict_msnbc = fetch_q_ids_docs('msnbc_new')\n",
    "docs_dict_aquaint = fetch_q_ids_docs('aquaint_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10316, 21)\n",
      "(10316, 13)\n",
      "739\n",
      "\n",
      "(7410, 21)\n",
      "(7410, 13)\n",
      "727\n"
     ]
    }
   ],
   "source": [
    "d_msnbc_ctx = fetch_all_features('basic_fea_ctx', 'test', 'msnbc_new')\n",
    "d_msnbc_ctx_raw = get_feature_results('basic_fea_ctx', 'test', 'msnbc_new')\n",
    "d_msnbc_coref = fetch_all_features('basic_fea_coref', 'test', 'msnbc_new')\n",
    "d_msnbc_total = get_total_mentions('msnbc_new', 'test')\n",
    "\n",
    "d_aquaint_ctx = fetch_all_features('basic_fea_ctx', 'test', 'aquaint_new')\n",
    "d_aquaint_ctx_raw = get_feature_results('basic_fea_ctx', 'test', 'aquaint_new')\n",
    "d_aquaint_coref = fetch_all_features('basic_fea_coref', 'test', 'aquaint_new')\n",
    "d_aquaint_total = get_total_mentions('aquaint_new', 'test')\n",
    "\n",
    "print(d_msnbc_ctx.shape)\n",
    "print(d_msnbc_coref.shape)\n",
    "print(d_msnbc_total)\n",
    "print()\n",
    "print(d_aquaint_ctx.shape)\n",
    "print(d_aquaint_coref.shape)\n",
    "print(d_aquaint_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "(10316, 32)\n",
      "(7410, 32)\n"
     ]
    }
   ],
   "source": [
    "d_msnbc_ctx_coref = combine_features(d_msnbc_ctx, d_msnbc_coref)\n",
    "d_aquaint_ctx_coref = combine_features(d_aquaint_ctx, d_aquaint_coref)\n",
    "\n",
    "print(d_msnbc_ctx_coref.shape)\n",
    "print(d_aquaint_ctx_coref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-19 14:49:08.349804 Loading model: 4900_6_ctx\n",
      "2019-08-19 14:49:08.370359 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.6278755074424899, corr_num: 464\n",
      "2019-08-19 14:49:08.662315 Evaluation finished\n",
      "2019-08-19 14:49:08.667954 Loading model: 4900_6_ctx_coref\n",
      "2019-08-19 14:49:08.693380 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.7117726657645467, corr_num: 526\n",
      "2019-08-19 14:49:09.003979 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('ctx', d_msnbc_ctx, d_msnbc_total, n_estimators=[4900], max_depths=[6])\n",
    "process('ctx_coref', d_msnbc_ctx_coref, d_msnbc_total, n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-19 14:49:24.595869 Loading model: 4900_6_ctx\n",
      "2019-08-19 14:49:24.616475 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.53232462173315, corr_num: 387\n",
      "2019-08-19 14:49:24.859625 Evaluation finished\n",
      "2019-08-19 14:49:24.865911 Loading model: 4900_6_ctx_coref\n",
      "2019-08-19 14:49:24.884611 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.5281980742778541, corr_num: 384\n",
      "2019-08-19 14:49:25.100430 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('ctx', d_aquaint_ctx, d_aquaint_total, n_estimators=[4900], max_depths=[6])\n",
    "process('ctx_coref', d_aquaint_ctx_coref, d_aquaint_total, n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups: 543\n"
     ]
    }
   ],
   "source": [
    "model = load_model('4900_6_ctx_coref')\n",
    "save_local_model_predictions(model, d_msnbc_ctx_coref, d_msnbc_ctx_raw, docs_dict_msnbc, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10316, 24)\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "(10316, 54)\n"
     ]
    }
   ],
   "source": [
    "d_msnbc_coh = fetch_all_features('basic_fea_coh', 'test', 'msnbc_new')\n",
    "print(d_msnbc_coh.shape)\n",
    "d_msnbc_ctx_coref_coh = combine_features(d_msnbc_ctx_coref, d_msnbc_coh)\n",
    "print(d_msnbc_ctx_coref_coh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-19 17:45:32.544743 Loading model: 4900_6_ctx_coref_coh\n",
      "2019-08-19 17:45:32.574501 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.7090663058186739, corr_num: 524\n",
      "2019-08-19 17:45:32.933873 Evaluation finished\n",
      "2019-08-19 17:45:32.941460 Loading model: 4900_6_ctx_coref_coh_global3\n",
      "2019-08-19 17:45:32.969372 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.7036535859269283, corr_num: 520\n",
      "2019-08-19 17:45:33.331261 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('ctx_coref_coh', d_msnbc_ctx_coref_coh, d_msnbc_total, n_estimators=[4900], max_depths=[6])\n",
    "process('ctx_coref_coh_global3', d_msnbc_ctx_coref_coh, d_msnbc_total, n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups: 422\n"
     ]
    }
   ],
   "source": [
    "model = load_model('4900_6_ctx_coref')\n",
    "save_local_model_predictions(model, d_aquaint_ctx_coref, d_aquaint_ctx_raw, docs_dict_aquaint, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_acquaint_coh = fetch_all_features('basic_fea_coh', 'test', 'acquaint_new')\n",
    "print(d_acquaint_coh.shape)\n",
    "d_acquaint_ctx_coref_coh = combine_features(d_acquaint_ctx_coref, d_msnbc_coh)\n",
    "print(d_msnbc_ctx_coref_coh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process('ctx_coref_coh', d_msnbc_ctx_coref_coh, d_msnbc_total, n_estimators=[4900], max_depths=[6])\n",
    "process('ctx_coref_coh_global3', d_msnbc_ctx_coref_coh, d_msnbc_total, n_estimators=[4900], max_depths=[6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
