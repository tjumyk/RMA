{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import redis\n",
    "import json\n",
    "import time\n",
    "import sklearn.utils\n",
    "import psycopg2\n",
    "from psycopg2.sql import Identifier, SQL\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = redis.Redis(password='3oSYTdtZjsuSigRWLcG6VJt9gm4IMvYjQiqsSuGcAc-U4gMNpWGERAevXi9_SHNrn19piz7bBJG0iTLgx7DvknLHTECcHYrqmWb2rsuCWs89svKmhKDD_aMYaXq8IhSeg_89ooPZb0AqLRyR1-fa1zVjrh2UuV0sWFGSk5SjtW0', \n",
    "                   host='localhost', port=7380, decode_responses=True)\n",
    "conn_psql_kongzi2 = psycopg2.connect(database='aida', user='zding', password='dingzishuo', host='localhost',\n",
    "                                     port=5432)\n",
    "conn_psql = psycopg2.connect(database='zding', user='zding', password='dingzishuo', host='localhost',\n",
    "                                     port=5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_results(exp_id, d_t, data_source='aida_conll'):\n",
    "    fea_res = conn.lrange('result:::::' + str(exp_id) + ':::::' + data_source + ':::::' + str(d_t), 0, -1)\n",
    "    fea_vecs = [json.loads(res) for res in fea_res]\n",
    "    return fea_vecs\n",
    "\n",
    "def get_dataset_info(exp_id, d_t, data_source='aida_conll', can_size=50):\n",
    "    valid_mens_size = conn.scard('valid_qry_ids:::::' + str(exp_id) + ':::::' + data_source + ':::::' + str(d_t))\n",
    "    no_g_can_info_size = conn.scard('no_g_candidate_info:::::' + str(exp_id) + ':::::' + data_source + ':::::' + str(d_t))\n",
    "    missed_g_candidate_size = conn.scard('missed_g_candidate:::::' + str(exp_id) + ':::::' + data_source + \n",
    "                                         ':::::' + str(d_t) + ':::::' + str(can_size))\n",
    "    no_candidate_size = conn.scard('no_candidate:::::' + str(exp_id) + ':::::' + data_source + ':::::' + str(d_t))\n",
    "    return valid_mens_size, no_candidate_size, no_g_can_info_size, missed_g_candidate_size\n",
    "\n",
    "def fetch_all_features(exp_id, data_type, data_source='aida_conll'):\n",
    "    res_feas = get_feature_results(exp_id, data_type, data_source)\n",
    "    res_feas_ids = [[res[0].strip('(').split(', ')[0]] + res[1:-1] + [res[-1]] for res in res_feas]\n",
    "    res_feas_ids = np.array(res_feas_ids, dtype=np.float64)\n",
    "    return res_feas_ids\n",
    "\n",
    "def fetch_all_features_delete_max_prior(exp_id, data_type, data_source='aida_conll'):\n",
    "    res_feas = get_feature_results(exp_id, data_type, data_source)\n",
    "    res_feas_ids = [[res[0].strip('(').split(', ')[0]] + res[1:2] + [res[2] if res[2]!=0 else res[1]] + res[3:] for res in res_feas]\n",
    "    res_feas_ids = np.array(res_feas_ids, dtype=np.float64)\n",
    "    return res_feas_ids\n",
    "\n",
    "def trans_data(data):\n",
    "    d_np = data[:, 1:-1]\n",
    "    #print(d_np)\n",
    "    d_labels = data[:, -1]\n",
    "    #print(d_labels)\n",
    "    idxs = np.where(d_labels == 1)[0]\n",
    "    d_groups = np.append(np.delete(idxs, 0), len(d_labels)) - idxs\n",
    "    xgb_data = xgb.DMatrix(data=d_np, label=d_labels)\n",
    "    xgb_data.set_group(d_groups)\n",
    "    return xgb_data\n",
    "\n",
    "def combine_features(original_feas, new_features):\n",
    "    men_id_feas_dict = defaultdict(list)\n",
    "    print(\"Building idx for new features...\")\n",
    "    for fea in new_features:\n",
    "        men_id_feas_dict[fea[0]].append(fea)\n",
    "    #for k, v in men_id_feas_dict.items():\n",
    "    #    print(k)\n",
    "    #    print(v)\n",
    "    comb_feas = []\n",
    "    pre_men_id = 0\n",
    "    print(\"Combine original and new features...\")\n",
    "    for fea_idx, fea in enumerate(original_feas):\n",
    "        #print('Processing: [%d] %s' % (fea_idx, fea))\n",
    "        if pre_men_id == fea[0]:\n",
    "            #print('skip')\n",
    "            continue\n",
    "        else:\n",
    "            pre_men_id = fea[0]\n",
    "            fea_size = len(men_id_feas_dict[fea[0]])\n",
    "            res = np.append(original_feas[fea_idx: fea_idx + fea_size, :-1], np.array(men_id_feas_dict[fea[0]])[:, 1:], axis=1)\n",
    "            #print('res:', res)\n",
    "            comb_feas.append(res)\n",
    "    return np.concatenate(comb_feas, axis=0)\n",
    "\n",
    "def evalerror(preds, dt, d_tal_size):\n",
    "    d_l = dt.get_label()\n",
    "    idxs = np.where(d_l == 1)[0]\n",
    "    d_groups = np.append(np.delete(idxs, 0), len(d_l)) - idxs\n",
    "    matched_ids = []\n",
    "    q_id = 0\n",
    "    for x in d_groups:\n",
    "        pre_res = preds[q_id: x + q_id]\n",
    "        if(preds[q_id] == max(pre_res)):\n",
    "            if len([x for x in pre_res if x == preds[q_id]]) == 1:\n",
    "                matched_ids.append(q_id)\n",
    "        q_id += x\n",
    "    precision = float(len(matched_ids)) / len(d_groups)\n",
    "    recall = float(len(matched_ids)) / d_tal_size\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return len(matched_ids), precision, recall, f1\n",
    "\n",
    "from collections import defaultdict\n",
    "def evalerror_detail_log(preds, dt, d_tal_size):\n",
    "    d_l = dt.get_label()\n",
    "    idxs = np.where(d_l == 1)[0]\n",
    "    d_groups = np.append(np.delete(idxs, 0), len(d_l)) - idxs\n",
    "    correct_results = {}\n",
    "    wrong_results = {}\n",
    "    duplicates_results = {}\n",
    "    group_info = {}\n",
    "    matched_ids = []\n",
    "    q_id = 0\n",
    "    for x in d_groups:\n",
    "        pre_res = preds[q_id: x + q_id]\n",
    "        if(preds[q_id] == max(pre_res)):\n",
    "            correct_results[q_id] = pre_res\n",
    "            if len([x for x in pre_res if x == preds[q_id]]) == 1:\n",
    "                matched_ids.append(q_id)\n",
    "            else:\n",
    "                duplicates_results[q_id] = pre_res\n",
    "        else:\n",
    "            wrong_results[q_id] = pre_res\n",
    "        q_id += x\n",
    "    precision = float(len(matched_ids)) / len(d_groups)\n",
    "    recall = float(len(matched_ids)) / d_tal_size\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return len(matched_ids), precision, recall, f1, correct_results, wrong_results, duplicates_results\n",
    "\n",
    "from collections import defaultdict\n",
    "from ast import literal_eval\n",
    "def get_groups_results(preds, dt, res_features):\n",
    "    d_l = dt.get_label()\n",
    "    idxs = np.where(d_l == 1)[0]\n",
    "    d_groups = np.append(np.delete(idxs, 0), len(d_l)) - idxs\n",
    "#     correct_results = {}\n",
    "#     wrong_results = {}\n",
    "#     duplicates_results = {}\n",
    "#     group_info = {}\n",
    "#     matched_ids = []\n",
    "    correct_res_groups = []\n",
    "    wrong_res_groups = []\n",
    "    dup_res_groups = []\n",
    "    q_id = 0\n",
    "    for x in d_groups:\n",
    "        pre_res = preds[q_id: x + q_id]\n",
    "        pre_res_feas = res_features[q_id: x + q_id]\n",
    "        pred_q_id, pred_ent = literal_eval(res_features[q_id+np.argmax(pre_res)][0])\n",
    "        \n",
    "        if(preds[q_id] == max(pre_res)):\n",
    "            correct_res_groups.append([pred_q_id, pred_ent])\n",
    "        else:\n",
    "            wrong_res_groups.append([pred_q_id, pred_ent])\n",
    "        q_id += x\n",
    "#     precision = float(len(matched_ids)) / len(d_groups)\n",
    "#     recall = float(len(matched_ids)) / d_tal_size\n",
    "#     f1 = 2 * precision * recall / (precision + recall)\n",
    "    return correct_res_groups, wrong_res_groups\n",
    "\n",
    "def fetch_inlinks_by_ent(ent):\n",
    "    cur = conn_psql_kongzi2.cursor()\n",
    "    sql = \"SELECT _id FROM wikipedia_links_2014 WHERE target=%s;\"\n",
    "    cur.execute(sql, (ent,))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    return rows\n",
    "\n",
    "# fetch an entity's outlinks with duplicates\n",
    "def fetch_outlinks_by_ent(ent):\n",
    "    cur = conn_psql_kongzi2.cursor()\n",
    "    sql = \"SELECT target FROM wikipedia_links_2014 WHERE _id=%s;\"\n",
    "    cur.execute(sql, (ent,))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    return rows\n",
    "\n",
    "def fetch_entity_by_mention_emnlp17(mention):\n",
    "    # print(mention)\n",
    "    cur = conn_psql_kongzi2.cursor()\n",
    "    # do a PostgreSQL join to select the entity namestring from the tables dictionary and entity_ids\n",
    "    sql = \"SELECT entity, prior FROM men_ent_dict_emnlp2017 WHERE men_ent_dict_emnlp2017.mention = (E\\'%s\\') ORDER BY prior DESC;\"\n",
    "    cur.execute(sql % mention.replace(\"'\", \"\\\\'\"))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    return rows\n",
    "\n",
    "def fetch_inlinks_redis(ent, link_type='inlinks'):\n",
    "    inlinks = conn.hmget(link_type, ent)[0]\n",
    "    return json.loads(inlinks) if inlinks else []\n",
    "\n",
    "def has_inlinks_redis(ent, link_type='inlinks'):\n",
    "    inlinks = conn.hexists(link_type, ent)\n",
    "    return inlinks\n",
    "\n",
    "def save_inlinks_redis(ent, inlinks, link_type='inlinks'):\n",
    "    conn.hset(link_type, ent, json.dumps(inlinks))\n",
    "    \n",
    "def fetch_outlinks_redis(ent, link_type='outlinks'):\n",
    "    outlinks = conn.hmget(link_type, ent)[0]\n",
    "    return json.loads(outlinks) if outlinks else []\n",
    "\n",
    "def has_outlinks_redis(ent, link_type='outlinks'):\n",
    "    inlinks = conn.hexists(link_type, ent)\n",
    "    return inlinks\n",
    "\n",
    "def save_outlinks_redis(ent, outlinks, link_type='outlinks'):\n",
    "    conn.hset(link_type, ent, json.dumps(outlinks))\n",
    "    \n",
    "def check_links_between_ents(ent_1, ent_2, bidirection=False):\n",
    "    wiki_pre_str = 'en.wikipedia.org/wiki/'\n",
    "    inlinks_ent_1 = fetch_inlinks_redis(ent_1, link_type='inlinks')\n",
    "    if not inlinks_ent_1 and not has_inlinks_redis(ent_1):\n",
    "        print(\"PostgreSQL: fetching inlinks for entity {}...\".format(ent_1))\n",
    "        wiki_ents_1 = wiki_pre_str + ent_1\n",
    "        inlinks_ent_1_db = fetch_inlinks_by_ent(wiki_ents_1)\n",
    "        inlinks_ent_1 = [x[0].replace(wiki_pre_str, '') for x in inlinks_ent_1_db]\n",
    "        print(\"Redis: caching inlinks for entity {}...\".format(ent_1))\n",
    "        save_inlinks_redis(ent_1, inlinks_ent_1)\n",
    "    inlinks_ent_2 = fetch_inlinks_redis(ent_2, link_type='inlinks')\n",
    "    if not inlinks_ent_2 and not has_inlinks_redis(ent_2):        \n",
    "        wiki_ents_2 = wiki_pre_str + ent_2\n",
    "        print(\"PostgreSQL: fetching inlinks for entity {}...\".format(ent_2))\n",
    "        inlinks_ent_2_db = fetch_inlinks_by_ent(wiki_ents_2)\n",
    "        inlinks_ent_2 = [x[0].replace(wiki_pre_str, '') for x in inlinks_ent_2_db]\n",
    "        print(\"Redis: caching inlinks for entity {}...\".format(ent_2))\n",
    "        save_inlinks_redis(ent_2, inlinks_ent_2)\n",
    "    return (ent_1 in inlinks_ent_2 or ent_2 in inlinks_ent_1) if not bidirection else (ent_1 in inlinks_ent_2 and ent_2 in inlinks_ent_1)\n",
    "\n",
    "def get_links_by_ent(ent, link_type='inlinks'):\n",
    "    wiki_pre_str = 'en.wikipedia.org/wiki/'\n",
    "    if link_type == 'inlinks':\n",
    "        inlinks_ent = fetch_inlinks_redis(ent, link_type='inlinks')\n",
    "        if not inlinks_ent and not has_inlinks_redis(ent):\n",
    "#             print(\"PostgreSQL: fetching inlinks for entity {}...\".format(ent))\n",
    "            wiki_ents = wiki_pre_str + ent\n",
    "            inlinks_ent_db = fetch_inlinks_by_ent(wiki_ents)\n",
    "            inlinks_ent = [x[0].replace(wiki_pre_str, '') for x in inlinks_ent_db]\n",
    "            print(\"Redis: caching inlinks for entity {}...\".format(ent))\n",
    "            save_inlinks_redis(ent, inlinks_ent)\n",
    "        return inlinks_ent\n",
    "    if link_type == 'outlinks':\n",
    "        outlinks_ent = fetch_outlinks_redis(ent)\n",
    "        if not outlinks_ent and not has_outlinks_redis(ent):\n",
    "#             print(\"PostgreSQL: fetching outlinks for entity {}...\".format(ent))\n",
    "            wiki_ents = wiki_pre_str + ent\n",
    "            outlinks_ent_db = fetch_outlinks_by_ent(wiki_ents)\n",
    "            outlinks_ent = [x[0].replace(wiki_pre_str, '') for x in outlinks_ent_db]\n",
    "            print(\"Redis: caching outlinks for entity {}...\".format(ent))\n",
    "            save_outlinks_redis(ent, outlinks_ent)\n",
    "        return outlinks_ent\n",
    "    \n",
    "def fetch_ents_by_doc_redis(doc_id):\n",
    "    id_ents = conn.hmget('doc-predicted-ents-coref-new', doc_id)[0]\n",
    "    return json.loads(id_ents) if id_ents else []\n",
    "\n",
    "## Normalized Google Distance\n",
    "import math\n",
    "def ngd_similarity(ents_s, ents_t, index_size = 6274625):\n",
    "    ent_sets_s = set(ents_s)\n",
    "    ent_sets_t = set(ents_t)\n",
    "    min_links, max_links = min(len(ent_sets_s), len(ent_sets_t)), max(len(ent_sets_s), len(ent_sets_t))\n",
    "    com_links = len(ent_sets_s & ent_sets_t)\n",
    "    if min_links and max_links and com_links:\n",
    "        return 1 - (math.log(max_links) - math.log(com_links))/ (math.log(index_size) - math.log(min_links))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# PMI\n",
    "def pmi_similarity(ents_s, ents_t, index_size = 6274625, normalize=False):\n",
    "    ent_sets_s = set(ents_s)\n",
    "    ent_sets_t = set(ents_t)\n",
    "    s_links, t_links = len(ent_sets_s), len(ent_sets_t)\n",
    "    com_links = len(ent_sets_s & ent_sets_t)\n",
    "    p_s = s_links / index_size\n",
    "    p_t = t_links / index_size\n",
    "    p_c = com_links / index_size\n",
    "    print(p_s, p_t, p_c)\n",
    "    if p_s and p_t and p_c:\n",
    "        return p_c/(p_s * p_t) if not normalize else p_c / (p_s * p_t) / min(1/p_s, 1/p_t)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_path = '.'\n",
    "def save_model(model, name):\n",
    "    if not os.path.exists(model_dir_path):\n",
    "        os.makedirs(model_dir_path)\n",
    "    model_path = os.path.join(model_dir_path, '%s.mdl' % name)\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "def load_model(name):\n",
    "    model_path = os.path.join(model_dir_path, '%s.mdl' % name)\n",
    "    with open(model_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(process_name ,train_set, testa_set, testb_set, train_total, testa_total, testb_total, \n",
    "            n_estimators, max_depths):\n",
    "    dtrain_xgboost = trans_data(train_set)\n",
    "    dtest_a_xgboost = trans_data(testa_set)\n",
    "    dtest_b_xgboost = trans_data(testb_set)\n",
    "\n",
    "#     train_true_labels = train_set[:, -1].nonzero()[0].size\n",
    "#     testa_true_labels = testa_set[:, -1].nonzero()[0].size\n",
    "#     testb_true_labels = testb_set[:, -1].nonzero()[0].size\n",
    "    \n",
    "#     print('True labels:', train_true_labels, testa_true_labels, testb_true_labels)\n",
    "    \n",
    "    for x in n_estimators:\n",
    "        num_round = x\n",
    "        for dep in max_depths:\n",
    "            model_name = '%d_%d_%s' % (num_round, dep, process_name)\n",
    "            print(datetime.now(), 'Loading model: %s' % model_name)\n",
    "            bst = load_model(model_name)\n",
    "\n",
    "            print(datetime.now(), 'Start evaluation')\n",
    "            preds = bst.predict(dtrain_xgboost)\n",
    "            a = evalerror_detail_log(preds, dtrain_xgboost, train_total)\n",
    "            print(\"n_estimators: {}, max_depth: {}, acc_training: {}, corr_num: {}\".format(num_round, dep, a[2], a[0]))\n",
    "            preds = bst.predict(dtest_a_xgboost)\n",
    "            a = evalerror_detail_log(preds, dtest_a_xgboost, testa_total)\n",
    "            print(\"n_estimators: {}, max_depth: {}, acc_validation: {}, corr_num: {}\".format(num_round, dep, a[2], a[0]))\n",
    "            preds = bst.predict(dtest_b_xgboost)\n",
    "            a = evalerror_detail_log(preds, dtest_b_xgboost, testb_total)\n",
    "            print(\"n_estimators: {}, max_depth: {}, acc_test: {}, corr_num: {}\".format(num_round, dep, a[2], a[0]))\n",
    "            print(datetime.now(), 'Evaluation finished')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_mentions(data_source, data_type) -> int:\n",
    "    with conn_psql.cursor() as cur:\n",
    "        sql = SQL(\"select count(*) from {} where annotation != 'NIL' and type=%s\").format(Identifier(data_source))\n",
    "        cur.execute(sql, (data_type,))\n",
    "        return cur.fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18541 4791 4485\n"
     ]
    }
   ],
   "source": [
    "d_train_total = get_total_mentions('aida_conll', 'train')\n",
    "d_test_a_total = get_total_mentions('aida_conll', 'testa')\n",
    "d_test_b_total = get_total_mentions('aida_conll', 'testb')\n",
    "\n",
    "print(d_train_total, d_test_a_total, d_test_b_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_ctx = fetch_all_features('basic_fea_ctx', 'train', 'aida_conll')\n",
    "d_test_a_ctx = fetch_all_features('basic_fea_ctx', 'testa', 'aida_conll')\n",
    "d_test_b_ctx = fetch_all_features('basic_fea_ctx', 'testb', 'aida_conll')\n",
    "d_train_coref = fetch_all_features('basic_fea_coref', 'train', 'aida_conll')\n",
    "d_test_a_coref = fetch_all_features('basic_fea_coref', 'testa', 'aida_conll')\n",
    "d_test_b_coref = fetch_all_features('basic_fea_coref', 'testb', 'aida_conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531835, 21)\n",
      "(136259, 21)\n",
      "(131893, 21)\n",
      "(531835, 13)\n",
      "(136259, 13)\n",
      "(131893, 13)\n"
     ]
    }
   ],
   "source": [
    "print(d_train_ctx.shape)\n",
    "print(d_test_a_ctx.shape)\n",
    "print(d_test_b_ctx.shape)\n",
    "print(d_train_coref.shape)\n",
    "print(d_test_a_coref.shape)\n",
    "print(d_test_b_coref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531835, 17)\n",
      "(136259, 17)\n",
      "(131893, 17)\n",
      "(531835, 10)\n",
      "(136259, 10)\n",
      "(131893, 10)\n"
     ]
    }
   ],
   "source": [
    "d_train_ctx_simplified = fetch_all_features('basic_fea_ctx_simplified', 'train', 'aida_conll')\n",
    "d_test_a_ctx_simplified = fetch_all_features('basic_fea_ctx_simplified', 'testa', 'aida_conll')\n",
    "d_test_b_ctx_simplified = fetch_all_features('basic_fea_ctx_simplified', 'testb', 'aida_conll')\n",
    "d_train_coref_simplified = fetch_all_features('basic_fea_coref_simplified', 'train', 'aida_conll')\n",
    "d_test_a_coref_simplified = fetch_all_features('basic_fea_coref_simplified', 'testa', 'aida_conll')\n",
    "d_test_b_coref_simplified = fetch_all_features('basic_fea_coref_simplified', 'testb', 'aida_conll')\n",
    "\n",
    "# #hotfix\n",
    "# indices = np.arange(d_train_ctx_simplified.shape[-1])\n",
    "# indices[2] = 4\n",
    "# indices[4] = 2\n",
    "# d_train_ctx_simplified = d_train_ctx_simplified[:, indices]\n",
    "# d_test_a_ctx_simplified = d_test_a_ctx_simplified[:, indices]\n",
    "# d_test_b_ctx_simplified = d_test_b_ctx_simplified[:, indices]\n",
    "\n",
    "print(d_train_ctx_simplified.shape)\n",
    "print(d_test_a_ctx_simplified.shape)\n",
    "print(d_test_b_ctx_simplified.shape)\n",
    "print(d_train_coref_simplified.shape)\n",
    "print(d_test_a_coref_simplified.shape)\n",
    "print(d_test_b_coref_simplified.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n"
     ]
    }
   ],
   "source": [
    "d_train_ctx_coref  = combine_features(d_train_ctx, d_train_coref)\n",
    "d_test_a_ctx_coref = combine_features(d_test_a_ctx, d_test_a_coref)\n",
    "d_test_b_ctx_coref = combine_features(d_test_b_ctx, d_test_b_coref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531835, 32)\n",
      "(136259, 32)\n",
      "(131893, 32)\n"
     ]
    }
   ],
   "source": [
    "print(d_train_ctx_coref.shape)\n",
    "print(d_test_a_ctx_coref.shape)\n",
    "print(d_test_b_ctx_coref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "(531835, 25)\n",
      "(136259, 25)\n",
      "(131893, 25)\n"
     ]
    }
   ],
   "source": [
    "d_train_ctx_coref_simplified  = combine_features(d_train_ctx_simplified, d_train_coref_simplified)\n",
    "d_test_a_ctx_coref_simplified = combine_features(d_test_a_ctx_simplified, d_test_a_coref_simplified)\n",
    "d_test_b_ctx_coref_simplified = combine_features(d_test_b_ctx_simplified, d_test_b_coref_simplified)\n",
    "print(d_train_ctx_coref_simplified.shape)\n",
    "print(d_test_a_ctx_coref_simplified.shape)\n",
    "print(d_test_b_ctx_coref_simplified.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_coh = fetch_all_features('basic_fea_coh', 'train', 'aida_conll')\n",
    "d_test_a_coh = fetch_all_features('basic_fea_coh', 'testa', 'aida_conll')\n",
    "d_test_b_coh = fetch_all_features('basic_fea_coh', 'testb', 'aida_conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531835, 24)\n",
      "(136259, 24)\n",
      "(131893, 24)\n"
     ]
    }
   ],
   "source": [
    "print(d_train_coh.shape)\n",
    "print(d_test_a_coh.shape)\n",
    "print(d_test_b_coh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n"
     ]
    }
   ],
   "source": [
    "d_train_ctx_coref_coh  = combine_features(d_train_ctx_coref, d_train_coh)\n",
    "d_test_a_ctx_coref_coh = combine_features(d_test_a_ctx_coref, d_test_a_coh)\n",
    "d_test_b_ctx_coref_coh = combine_features(d_test_b_ctx_coref, d_test_b_coh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531835, 54)\n",
      "(136259, 54)\n",
      "(131893, 54)\n"
     ]
    }
   ],
   "source": [
    "print(d_train_ctx_coref_coh.shape)\n",
    "print(d_test_a_ctx_coref_coh.shape)\n",
    "print(d_test_b_ctx_coref_coh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "Building idx for new features...\n",
      "Combine original and new features...\n",
      "(531835, 47)\n",
      "(136259, 47)\n",
      "(131893, 47)\n"
     ]
    }
   ],
   "source": [
    "d_train_ctx_coref_coh_simplified  = combine_features(d_train_ctx_coref_simplified, d_train_coh)\n",
    "d_test_a_ctx_coref_coh_simplified = combine_features(d_test_a_ctx_coref_simplified, d_test_a_coh)\n",
    "d_test_b_ctx_coref_coh_simplified = combine_features(d_test_b_ctx_coref_simplified, d_test_b_coh)\n",
    "print(d_train_ctx_coref_coh_simplified.shape)\n",
    "print(d_test_a_ctx_coref_coh_simplified.shape)\n",
    "print(d_test_b_ctx_coref_coh_simplified.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531835, 23)\n",
      "(136259, 23)\n",
      "(131893, 23)\n"
     ]
    }
   ],
   "source": [
    "d_train_ctx_no_poly = fetch_all_features('basic_no_poly', 'train', 'aida_conll')\n",
    "d_test_a_ctx_no_poly = fetch_all_features('basic_no_poly', 'testa', 'aida_conll')\n",
    "d_test_b_ctx_no_poly = fetch_all_features('basic_no_poly', 'testb', 'aida_conll')\n",
    "print(d_train_ctx_no_poly.shape)\n",
    "print(d_test_a_ctx_no_poly.shape)\n",
    "print(d_test_b_ctx_no_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531835, 17)\n",
      "(136259, 17)\n",
      "(131893, 17)\n"
     ]
    }
   ],
   "source": [
    "d_train_ctx_poly = fetch_all_features('basic_poly', 'train', 'aida_conll')\n",
    "d_test_a_ctx_poly = fetch_all_features('basic_poly', 'testa', 'aida_conll')\n",
    "d_test_b_ctx_poly = fetch_all_features('basic_poly', 'testb', 'aida_conll')\n",
    "print(d_train_ctx_poly.shape)\n",
    "print(d_test_a_ctx_poly.shape)\n",
    "print(d_test_b_ctx_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-17 22:16:56.416965 Loading model: 4900_6_coref_new\n",
      "2019-08-17 22:16:56.443034 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_training: 0.782589935817917, corr_num: 14510\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.7422250052181173, corr_num: 3556\n",
      "n_estimators: 4900, max_depth: 6, acc_test: 0.7547380156075808, corr_num: 3385\n",
      "2019-08-17 22:17:22.063862 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('coref_new', d_train_ctx_coref_simplified, d_test_a_ctx_coref_simplified, d_test_b_ctx_coref_simplified, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 14:17:49.062497 Loading model: 4900_6_coref_new_emnlp17\n",
      "2019-08-16 14:17:49.098575 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_training: 0.8564802329971415, corr_num: 15880\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.8102692548528491, corr_num: 3882\n",
      "n_estimators: 4900, max_depth: 6, acc_test: 0.7933110367892977, corr_num: 3558\n",
      "2019-08-16 14:18:11.543792 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('coref_new_emnlp17', d_train_ctx_coref, d_test_a_ctx_coref, d_test_b_ctx_coref, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-17 22:17:22.277329 Loading model: 4900_6_coref_new_emnlp17_for_review\n",
      "2019-08-17 22:17:22.307015 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_training: 0.7369073944231703, corr_num: 13663\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.7006887914840325, corr_num: 3357\n",
      "n_estimators: 4900, max_depth: 6, acc_test: 0.711705685618729, corr_num: 3192\n",
      "2019-08-17 22:17:45.479320 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('coref_new_emnlp17_for_review', \n",
    "        d_train_ctx_coref_simplified, d_test_a_ctx_coref_simplified, d_test_b_ctx_coref_simplified, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-17 22:17:45.899298 Loading model: 4900_6_coref_new_coh\n",
      "2019-08-17 22:17:45.928679 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_training: 0.8916994768351222, corr_num: 16533\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.8515967438948028, corr_num: 4080\n",
      "n_estimators: 4900, max_depth: 6, acc_test: 0.853288740245262, corr_num: 3827\n",
      "2019-08-17 22:18:08.045637 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('coref_new_coh', \n",
    "        d_train_ctx_coref_coh_simplified, d_test_a_ctx_coref_coh_simplified, d_test_b_ctx_coref_coh_simplified, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 14:22:41.290249 Loading model: 4900_6_coref_new_coh_emnlp17\n",
      "2019-08-16 14:22:41.319749 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_training: 0.9381371015587078, corr_num: 17394\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.8712168649551242, corr_num: 4174\n",
      "n_estimators: 4900, max_depth: 6, acc_test: 0.875139353400223, corr_num: 3925\n",
      "2019-08-16 14:23:01.949405 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('coref_new_coh_emnlp17', d_train_ctx_coref_coh, d_test_a_ctx_coref_coh, d_test_b_ctx_coref_coh, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-16 14:23:02.382560 Loading model: 4900_6_coref_new_coh_emnlp17_for_review\n",
      "2019-08-16 14:23:02.414279 Start evaluation\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51']\ntraining data did not have the following fields: f49, f48, f50, f47, f51, f42, f44, f45, f43, f46",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2e619de6c2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m process('coref_new_coh_emnlp17_for_review', d_train_ctx_coref_coh, d_test_a_ctx_coref_coh, d_test_b_ctx_coref_coh, \n\u001b[1;32m      2\u001b[0m         \u001b[0md_train_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_test_a_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_test_b_total\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         n_estimators=[4900], max_depths=[6])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-c0217921f235>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(process_name, train_set, testa_set, testb_set, train_total, testa_total, testb_total, n_estimators, max_depths)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Start evaluation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain_xgboost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevalerror_detail_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain_xgboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n_estimators: {}, max_depth: {}, acc_training: {}, corr_num: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/nerd/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/nerd/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1690\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51']\ntraining data did not have the following fields: f49, f48, f50, f47, f51, f42, f44, f45, f43, f46"
     ]
    }
   ],
   "source": [
    "process('coref_new_coh_emnlp17_for_review', d_train_ctx_coref_coh, d_test_a_ctx_coref_coh, d_test_b_ctx_coref_coh, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-17 22:18:08.243232 Loading model: 4900_6_no_poly\n",
      "2019-08-17 22:18:08.279071 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_training: 0.7490965967315679, corr_num: 13889\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.7278230014610728, corr_num: 3487\n",
      "n_estimators: 4900, max_depth: 6, acc_test: 0.7228539576365663, corr_num: 3242\n",
      "2019-08-17 22:18:32.961397 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('no_poly', d_train_ctx_simplified, d_test_a_ctx_simplified, d_test_b_ctx_simplified, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-17 22:18:33.116809 Loading model: 1700_8_no_poly\n",
      "2019-08-17 22:18:33.152419 Start evaluation\n",
      "n_estimators: 1700, max_depth: 8, acc_training: 0.743109864624346, corr_num: 13778\n",
      "n_estimators: 1700, max_depth: 8, acc_validation: 0.723022333542058, corr_num: 3464\n",
      "n_estimators: 1700, max_depth: 8, acc_test: 0.7072463768115942, corr_num: 3172\n",
      "2019-08-17 22:18:44.796798 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "process('no_poly', d_train_ctx_simplified, d_test_a_ctx_simplified, d_test_b_ctx_simplified, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[1700], max_depths=[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531835, 54)\n",
      "2019-08-17 22:18:45.249010 Loading model: 4900_6_global\n",
      "2019-08-17 22:18:45.275783 Start evaluation\n",
      "n_estimators: 4900, max_depth: 6, acc_training: 0.8781619114395124, corr_num: 16282\n",
      "n_estimators: 4900, max_depth: 6, acc_validation: 0.8265497808390733, corr_num: 3960\n",
      "n_estimators: 4900, max_depth: 6, acc_test: 0.8390189520624304, corr_num: 3763\n",
      "2019-08-17 22:19:06.947047 Evaluation finished\n"
     ]
    }
   ],
   "source": [
    "print(d_train_ctx_coref_coh.shape)\n",
    "process('global', \n",
    "        d_train_ctx_coref_coh_simplified, d_test_a_ctx_coref_coh_simplified, d_test_b_ctx_coref_coh_simplified, \n",
    "        d_train_total, d_test_a_total, d_test_b_total,\n",
    "        n_estimators=[4900], max_depths=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
